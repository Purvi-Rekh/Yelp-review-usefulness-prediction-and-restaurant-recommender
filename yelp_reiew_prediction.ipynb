{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark as ps\n",
    "from  pyspark.sql.types import StructType,StructField,StringType\n",
    "spark = (ps.sql.SparkSession\n",
    "         .builder\n",
    "         .master('local[4]')\n",
    "         .appName('lecture')\n",
    "         .getOrCreate()\n",
    "        )\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlcontext=SQLContext(sc)\n",
    "# df=sqlcontext.read.json('yelp_dataset/user.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "business=sqlcontext.read.json('yelp_dataset/business.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o42.json.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 37 in stage 1.0 failed 1 times, most recent failure: Lost task 37.0 in stage 1.0 (TID 41, localhost, executor driver): ExecutorLostFailure (executor driver exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 144521 ms\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)\n\tat org.apache.spark.sql.catalyst.json.JsonInferSchema$.infer(JsonInferSchema.scala:83)\n\tat org.apache.spark.sql.execution.datasources.json.TextInputJsonDataSource$$anonfun$inferFromDataset$1.apply(JsonDataSource.scala:109)\n\tat org.apache.spark.sql.execution.datasources.json.TextInputJsonDataSource$$anonfun$inferFromDataset$1.apply(JsonDataSource.scala:109)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.datasources.json.TextInputJsonDataSource$.inferFromDataset(JsonDataSource.scala:108)\n\tat org.apache.spark.sql.execution.datasources.json.TextInputJsonDataSource$.infer(JsonDataSource.scala:98)\n\tat org.apache.spark.sql.execution.datasources.json.JsonDataSource.inferSchema(JsonDataSource.scala:64)\n\tat org.apache.spark.sql.execution.datasources.json.JsonFileFormat.inferSchema(JsonFileFormat.scala:59)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$6.apply(DataSource.scala:180)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$6.apply(DataSource.scala:180)\n\tat scala.Option.orElse(Option.scala:289)\n\tat org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:179)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:373)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:223)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n\tat org.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:391)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-744fec657d45>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mreview\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msqlcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'yelp_dataset/review.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mjson\u001b[0;34m(self, path, schema, primitivesAsString, prefersDecimal, allowComments, allowUnquotedFieldNames, allowSingleQuotes, allowNumericLeadingZero, allowBackslashEscapingAnyCharacter, mode, columnNameOfCorruptRecord, dateFormat, timestampFormat, multiLine, allowUnquotedControlChars, lineSep, samplingRatio, dropFieldIfAllNull, encoding)\u001b[0m\n\u001b[1;32m    272\u001b[0m             \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o42.json.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 37 in stage 1.0 failed 1 times, most recent failure: Lost task 37.0 in stage 1.0 (TID 41, localhost, executor driver): ExecutorLostFailure (executor driver exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 144521 ms\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)\n\tat org.apache.spark.sql.catalyst.json.JsonInferSchema$.infer(JsonInferSchema.scala:83)\n\tat org.apache.spark.sql.execution.datasources.json.TextInputJsonDataSource$$anonfun$inferFromDataset$1.apply(JsonDataSource.scala:109)\n\tat org.apache.spark.sql.execution.datasources.json.TextInputJsonDataSource$$anonfun$inferFromDataset$1.apply(JsonDataSource.scala:109)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.datasources.json.TextInputJsonDataSource$.inferFromDataset(JsonDataSource.scala:108)\n\tat org.apache.spark.sql.execution.datasources.json.TextInputJsonDataSource$.infer(JsonDataSource.scala:98)\n\tat org.apache.spark.sql.execution.datasources.json.JsonDataSource.inferSchema(JsonDataSource.scala:64)\n\tat org.apache.spark.sql.execution.datasources.json.JsonFileFormat.inferSchema(JsonFileFormat.scala:59)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$6.apply(DataSource.scala:180)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$6.apply(DataSource.scala:180)\n\tat scala.Option.orElse(Option.scala:289)\n\tat org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:179)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:373)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:223)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n\tat org.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:391)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n"
     ]
    }
   ],
   "source": [
    "review=sqlcontext.read.json('yelp_dataset/review.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review.printSchema()\n",
    "#review[\"text\"].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# schema = StructType([StructField('json', StringType(), True)])\n",
    "# rdd = (review\n",
    "#   .select('json')\n",
    "#   .rdd\n",
    "#   .flatMap(lambda x: x)\n",
    "#   .flatMap(lambda x: json.loads(x))\n",
    "#   .map(lambda x: x.get('body'))\n",
    "# )\n",
    "# new_df = sql_context.createDataFrame(rdd, schema)\n",
    "# new_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review.createOrReplaceTempView(\"yelp\")\n",
    "business.createOrReplaceTempView(\"yelp_business\")\n",
    "#unique_business = spark.sql(\"select distinct business_id from yelp\")\n",
    "#unique_business.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_comment = spark.sql(\"select distinct text from yelp\")\n",
    "unique_comment.count()\n",
    "yelp_review = spark.sql(\"SELECT business_id,useful,text FROM yelp WHERE useful>=1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp_review.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp_review.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp_review.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp_review.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "df_out = yelp_review.groupBy(\"business_id\").agg(F.sum(\"useful\")).orderBy(\"sum(useful)\", ascending=False)\n",
    "df_out.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_out2 = yelp_review.groupBy(\"text\").agg(F.sum(\"business_id\"))\n",
    "df_out2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_out2 = yelp_review.select(\"business_id\").distinct().count()\n",
    "df_out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_buisness=yelp_review.select(\"business_id\").count()\n",
    "total_buisness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_buisness/df_out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "business.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "business_view=business.select(\"business_id\",\"name\",\"categories\",\"review_count\",\"stars\").show()#groupby(\"categories\").count(\"review_count\").orderBy(\"review_count\").show()\n",
    "business_view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#yelp_review_useful0 = spark.sql(\"SELECT business_id,useful,text FROM yelp WHERE useful=0\")\n",
    "#yelp_review_useful0_count = spark.sql(\"SELECT b.business_id,b.name,count(y.useful) FROM yelp as y inner join business_view b group by y.business_id having count(y.useful)=0\")\n",
    "\n",
    "#yelp_review_useful0_count.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type(yelp_business)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp_review_useful0.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buss_category = spark.sql(\"SELECT distinct business_id,categories from yelp_business where categories like '%India%'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#buss_category.show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#buss_category.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(buss_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.sql(select buss_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#business_view=business.select(\"business_id\",\"name\",\"categories\",\"review_count\",\"stars\").show()#groupby(\"categories\").count(\"review_count\").orderBy(\"review_count\").show()\n",
    "review_name_category = spark.sql(\"select categories,name,count(name) from yelp_business group by categories\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buss_category2 = spark.sql(\"SELECT distinct b.business_id,b.categories,r.useful,r.text from yelp_business b inner join yelp r on r.business_id = b.business_id where categories like '%India%'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_buss = spark.sql(\"SELECT distinct b.business_id,b.name,b.categories,r.useful,r.text,r.stars,r.funny,r.cool from yelp_business b inner join yelp r on r.business_id = b.business_id where categories like '%India%'\")\n",
    "new_buss.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "user=sqlcontext.read.json('yelp_dataset/user.json')\n",
    "user.createOrReplaceTempView(\"yelp_user\")\n",
    "user_data=spark.sql(\"select * from yelp_user\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#buss_category2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#buss_cat_useful = buss_category2.select(\"useful\")\n",
    "buss_cat_useful=buss_category2.filter(\"useful >= 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38041"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "buss_cat_useful.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#buss_cat_useful = buss_category2.select(\"useful\" ==0)\n",
    "buss_not_cat_useful=buss_category2.filter(\"useful == 0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41726"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "buss_not_cat_useful.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buss_category2.write.format(\"csv\").save(\"indian_review\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "indian_rest_review_prediction=buss_category2.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_indian_resto__buss_review = new_buss.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>name</th>\n",
       "      <th>categories</th>\n",
       "      <th>useful</th>\n",
       "      <th>text</th>\n",
       "      <th>stars</th>\n",
       "      <th>funny</th>\n",
       "      <th>cool</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>iE71iwcSljg3xm2GB2Y9aA</td>\n",
       "      <td>Marigold Maison- Paradise Valley</td>\n",
       "      <td>Indian, Restaurants</td>\n",
       "      <td>0</td>\n",
       "      <td>I have been here several times since my last u...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DJuQgsoj-qmU0i5iTAa4aA</td>\n",
       "      <td>Tadka Restaurant</td>\n",
       "      <td>Restaurants, Indian</td>\n",
       "      <td>0</td>\n",
       "      <td>We just tried to eat here and I'm pretty sure ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FlOvCwPUrRCicfbpd1tUIQ</td>\n",
       "      <td>Zauq</td>\n",
       "      <td>Restaurants, Food, Pakistani, Indian</td>\n",
       "      <td>0</td>\n",
       "      <td>Ok. I'm a regular at Zauq. Their breakfast spe...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Wrd9LAbc8IOoAS5H6uw9sw</td>\n",
       "      <td>Delhi Palace Cuisine of India</td>\n",
       "      <td>Pakistani, Indian, Buffets, Restaurants</td>\n",
       "      <td>0</td>\n",
       "      <td>You will probably have a hard time finding thi...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>H_YBTOS23PRhrjWtP7TeJw</td>\n",
       "      <td>Marigold Indian Bistro</td>\n",
       "      <td>Indian, Pakistani, Restaurants</td>\n",
       "      <td>3</td>\n",
       "      <td>My family loves Indian food and we have been e...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id                              name  \\\n",
       "0  iE71iwcSljg3xm2GB2Y9aA  Marigold Maison- Paradise Valley   \n",
       "1  DJuQgsoj-qmU0i5iTAa4aA                  Tadka Restaurant   \n",
       "2  FlOvCwPUrRCicfbpd1tUIQ                              Zauq   \n",
       "3  Wrd9LAbc8IOoAS5H6uw9sw     Delhi Palace Cuisine of India   \n",
       "4  H_YBTOS23PRhrjWtP7TeJw            Marigold Indian Bistro   \n",
       "\n",
       "                                categories  useful  \\\n",
       "0                      Indian, Restaurants       0   \n",
       "1                      Restaurants, Indian       0   \n",
       "2     Restaurants, Food, Pakistani, Indian       0   \n",
       "3  Pakistani, Indian, Buffets, Restaurants       0   \n",
       "4           Indian, Pakistani, Restaurants       3   \n",
       "\n",
       "                                                text  stars  funny  cool  \n",
       "0  I have been here several times since my last u...    5.0      0     0  \n",
       "1  We just tried to eat here and I'm pretty sure ...    1.0      1     0  \n",
       "2  Ok. I'm a regular at Zauq. Their breakfast spe...    4.0      0     0  \n",
       "3  You will probably have a hard time finding thi...    4.0      0     0  \n",
       "4  My family loves Indian food and we have been e...    5.0      0     0  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_indian_resto__buss_review.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_indian_resto__buss_review.to_csv(\"new_dataset_indian_resto_reiew\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>categories</th>\n",
       "      <th>useful</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9OHxN88qy_BSmY_8yqWQ5w</td>\n",
       "      <td>Indian, Halal, Vegetarian, Restaurants</td>\n",
       "      <td>1</td>\n",
       "      <td>We came here with a large group of family on a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SU56w479vUfFHsvmvQIf7A</td>\n",
       "      <td>Restaurants, Indian, Pakistani</td>\n",
       "      <td>1</td>\n",
       "      <td>The food was great. The flavors seemed authent...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>xA6L2w_TlbxHubb1xeBE8w</td>\n",
       "      <td>Food, Pakistani, Indian, Restaurants, Bakeries</td>\n",
       "      <td>1</td>\n",
       "      <td>I go back and forth on how I feel about Tamari...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>xA6L2w_TlbxHubb1xeBE8w</td>\n",
       "      <td>Food, Pakistani, Indian, Restaurants, Bakeries</td>\n",
       "      <td>1</td>\n",
       "      <td>Kodi vepudu... Is my favourite... will never m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>EfAqol3tWckyNrBMIooJmg</td>\n",
       "      <td>Arabian, Indian, Restaurants, Pakistani, Halal</td>\n",
       "      <td>1</td>\n",
       "      <td>This place is really good. My wife and I were ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id                                      categories  \\\n",
       "0  9OHxN88qy_BSmY_8yqWQ5w          Indian, Halal, Vegetarian, Restaurants   \n",
       "1  SU56w479vUfFHsvmvQIf7A                  Restaurants, Indian, Pakistani   \n",
       "2  xA6L2w_TlbxHubb1xeBE8w  Food, Pakistani, Indian, Restaurants, Bakeries   \n",
       "3  xA6L2w_TlbxHubb1xeBE8w  Food, Pakistani, Indian, Restaurants, Bakeries   \n",
       "4  EfAqol3tWckyNrBMIooJmg  Arabian, Indian, Restaurants, Pakistani, Halal   \n",
       "\n",
       "   useful                                               text  \n",
       "0       1  We came here with a large group of family on a...  \n",
       "1       1  The food was great. The flavors seemed authent...  \n",
       "2       1  I go back and forth on how I feel about Tamari...  \n",
       "3       1  Kodi vepudu... Is my favourite... will never m...  \n",
       "4       1  This place is really good. My wife and I were ...  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indian_rest_review_prediction.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def useful_not_useful(x):\n",
    "    if x == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "indian_rest_review_prediction[\"label\"]=indian_rest_review_prediction[\"useful\"].apply(lambda x:useful_not_useful(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>categories</th>\n",
       "      <th>useful</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9OHxN88qy_BSmY_8yqWQ5w</td>\n",
       "      <td>Indian, Halal, Vegetarian, Restaurants</td>\n",
       "      <td>1</td>\n",
       "      <td>We came here with a large group of family on a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SU56w479vUfFHsvmvQIf7A</td>\n",
       "      <td>Restaurants, Indian, Pakistani</td>\n",
       "      <td>1</td>\n",
       "      <td>The food was great. The flavors seemed authent...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>xA6L2w_TlbxHubb1xeBE8w</td>\n",
       "      <td>Food, Pakistani, Indian, Restaurants, Bakeries</td>\n",
       "      <td>1</td>\n",
       "      <td>I go back and forth on how I feel about Tamari...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>xA6L2w_TlbxHubb1xeBE8w</td>\n",
       "      <td>Food, Pakistani, Indian, Restaurants, Bakeries</td>\n",
       "      <td>1</td>\n",
       "      <td>Kodi vepudu... Is my favourite... will never m...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>EfAqol3tWckyNrBMIooJmg</td>\n",
       "      <td>Arabian, Indian, Restaurants, Pakistani, Halal</td>\n",
       "      <td>1</td>\n",
       "      <td>This place is really good. My wife and I were ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-iFvYhgysvjkxckCr42NRw</td>\n",
       "      <td>Restaurants, Food, Ethnic Food, Indian, Specia...</td>\n",
       "      <td>1</td>\n",
       "      <td>Totally amazed that this place offers free cha...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ohz8ljVxcZMo2bMGrBZfgw</td>\n",
       "      <td>Vegetarian, Nightlife, Indian, Salad, Pakistan...</td>\n",
       "      <td>1</td>\n",
       "      <td>The best Indian in Vegas, and perhaps the best...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>bumAFxitMRHKAxZMijvUYg</td>\n",
       "      <td>Caterers, Event Planning &amp; Services, Restauran...</td>\n",
       "      <td>1</td>\n",
       "      <td>Apparently this used to be a restaurant before...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ohz8ljVxcZMo2bMGrBZfgw</td>\n",
       "      <td>Vegetarian, Nightlife, Indian, Salad, Pakistan...</td>\n",
       "      <td>0</td>\n",
       "      <td>Nice little Indian restaurant near my hotel. H...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>iJfS02AOptwkaWD-vOVRew</td>\n",
       "      <td>Restaurants, Indian</td>\n",
       "      <td>0</td>\n",
       "      <td>My husband &amp; I LOVE this place!  Service, food...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>b4LxjVnFdOCqRKxfweaTqw</td>\n",
       "      <td>Indian, Restaurants</td>\n",
       "      <td>0</td>\n",
       "      <td>The butter chicken and naan were delicious! I ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Wrd9LAbc8IOoAS5H6uw9sw</td>\n",
       "      <td>Pakistani, Indian, Buffets, Restaurants</td>\n",
       "      <td>2</td>\n",
       "      <td>The lackluster service began when I walked in ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>b5J-EUKzmzj8U29Q6miV6Q</td>\n",
       "      <td>Pakistani, Indian, Buffets, Restaurants</td>\n",
       "      <td>2</td>\n",
       "      <td>We've been coming here for about 8 years, and ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>iE71iwcSljg3xm2GB2Y9aA</td>\n",
       "      <td>Indian, Restaurants</td>\n",
       "      <td>0</td>\n",
       "      <td>Love Love Love this place!!! Go for lunch thou...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>iE71iwcSljg3xm2GB2Y9aA</td>\n",
       "      <td>Indian, Restaurants</td>\n",
       "      <td>1</td>\n",
       "      <td>I wanted to try this place out based on some p...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               business_id                                         categories  \\\n",
       "0   9OHxN88qy_BSmY_8yqWQ5w             Indian, Halal, Vegetarian, Restaurants   \n",
       "1   SU56w479vUfFHsvmvQIf7A                     Restaurants, Indian, Pakistani   \n",
       "2   xA6L2w_TlbxHubb1xeBE8w     Food, Pakistani, Indian, Restaurants, Bakeries   \n",
       "3   xA6L2w_TlbxHubb1xeBE8w     Food, Pakistani, Indian, Restaurants, Bakeries   \n",
       "4   EfAqol3tWckyNrBMIooJmg     Arabian, Indian, Restaurants, Pakistani, Halal   \n",
       "5   -iFvYhgysvjkxckCr42NRw  Restaurants, Food, Ethnic Food, Indian, Specia...   \n",
       "6   ohz8ljVxcZMo2bMGrBZfgw  Vegetarian, Nightlife, Indian, Salad, Pakistan...   \n",
       "7   bumAFxitMRHKAxZMijvUYg  Caterers, Event Planning & Services, Restauran...   \n",
       "8   ohz8ljVxcZMo2bMGrBZfgw  Vegetarian, Nightlife, Indian, Salad, Pakistan...   \n",
       "9   iJfS02AOptwkaWD-vOVRew                                Restaurants, Indian   \n",
       "10  b4LxjVnFdOCqRKxfweaTqw                                Indian, Restaurants   \n",
       "11  Wrd9LAbc8IOoAS5H6uw9sw            Pakistani, Indian, Buffets, Restaurants   \n",
       "12  b5J-EUKzmzj8U29Q6miV6Q            Pakistani, Indian, Buffets, Restaurants   \n",
       "13  iE71iwcSljg3xm2GB2Y9aA                                Indian, Restaurants   \n",
       "14  iE71iwcSljg3xm2GB2Y9aA                                Indian, Restaurants   \n",
       "\n",
       "    useful                                               text  label  \n",
       "0        1  We came here with a large group of family on a...      1  \n",
       "1        1  The food was great. The flavors seemed authent...      1  \n",
       "2        1  I go back and forth on how I feel about Tamari...      1  \n",
       "3        1  Kodi vepudu... Is my favourite... will never m...      1  \n",
       "4        1  This place is really good. My wife and I were ...      1  \n",
       "5        1  Totally amazed that this place offers free cha...      1  \n",
       "6        1  The best Indian in Vegas, and perhaps the best...      1  \n",
       "7        1  Apparently this used to be a restaurant before...      1  \n",
       "8        0  Nice little Indian restaurant near my hotel. H...      0  \n",
       "9        0  My husband & I LOVE this place!  Service, food...      0  \n",
       "10       0  The butter chicken and naan were delicious! I ...      0  \n",
       "11       2  The lackluster service began when I walked in ...      1  \n",
       "12       2  We've been coming here for about 8 years, and ...      1  \n",
       "13       0  Love Love Love this place!!! Go for lunch thou...      0  \n",
       "14       1  I wanted to try this place out based on some p...      1  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indian_rest_review_prediction.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "india_review=indian_rest_review_prediction[[\"business_id\",\"text\",\"label\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "business_id    38041\n",
       "text           38041\n",
       "label          38041\n",
       "dtype: int64"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "india_review[india_review[\"label\"] == 1].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "business_id    41726\n",
       "text           41726\n",
       "label          41726\n",
       "dtype: int64"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "india_review[india_review[\"label\"] == 0].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import nltk\n",
    "\n",
    "#nltk.download(\"wordnet\")\n",
    "#nltk.download(\"stopwords\")\n",
    "import unicodedata\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "stopwords_ = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer() \n",
    "tf = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_accents(input_str):\n",
    "    nfkd_form = unicodedata.normalize('NFKD', input_str)\n",
    "    only_ascii = nfkd_form.encode('ASCII', 'ignore')\n",
    "    return only_ascii.decode()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing(review):\n",
    "    india_review[\"normalizes_text\"] = india_review[\"text\"].apply(lambda x:remove_accents(x))\n",
    "    #india_review[\"remove_html\"] = india_review['normalizes_text'].apply(lambda x: strip_html_tags(x))\n",
    "    india_review[\"remove_special\"] = india_review[\"normalizes_text\"].replace(r'[^A-Za-z0-9 ]+', '', regex=True)\n",
    "    india_review[\"token_text\"] = india_review[\"remove_special\"].apply(lambda x:[word.lower() for word in x.split(\" \")]) \n",
    "    india_review[\"remove_stop\"] = india_review[\"token_text\"].apply(lambda x:[word for word in x if word not  in stopwords_])\n",
    "    india_review[\"lemitize_text\"] = india_review[\"remove_stop\"].apply(lambda x: \" \".join([lemmatizer.lemmatize(word) for word in x]))\n",
    "   # review[\"clean_list\"] = review[\"lemitize_text\"].apply(lambda x:[word for word in x.split(\" \")])\n",
    "    return india_review\n",
    "     \n",
    "  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "new_india_review=pre_processing(india_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_india_review.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_counter(review):\n",
    "    countvect = CountVectorizer()\n",
    "    bag_of_words = countvect.fit_transform(review[\"lemitize_text\"])\n",
    "    sum_words = bag_of_words.sum(axis=0)\n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in countvect.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = word_counter(new_india_review)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = new_india_review[\"lemitize_text\"]\n",
    "X.head()\n",
    "y=new_india_review[\"label\"]\n",
    "tfidf = TfidfVectorizer()\n",
    "X_mat=tfidf.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_model(X,y,model):\n",
    "    \n",
    "    print(\"model is \",model)\n",
    "    X_train,X_test,y_train,y_test = train_test_split(X_mat,y,test_size=0.20)\n",
    "    #rf = RandomForestClassifier()\n",
    "    \n",
    "    \n",
    "    model.fit(X_train,y_train)\n",
    "    y_pred=model.predict(X_test)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    print(\"accuracy score = \",accuracy_score(y_test,y_pred))\n",
    "    print(\"accuracy score = \",confusion_matrix(y_test,y_pred))\n",
    "    print(\"precision = \",precision)\n",
    "    print(\"recall = \",recall)\n",
    "    print(\"f1 score = \",f1_score(y_test,y_pred))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score,f1_score\n",
    "\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "GNB = GaussianNB()\n",
    "MNB = MultinomialNB()\n",
    "BNB = BernoulliNB()\n",
    "lg = LogisticRegression()\n",
    "dt = DecisionTreeClassifier()\n",
    "\n",
    "# X = vectorizer.fit_transform(text_series)\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "#     model.fit(X_train, y_train)\n",
    "#     y_pred = model.predict(X_test)\n",
    "#     precision = precision_score(y_test, y_pred)\n",
    "#     recall = recall_score(y_test, y_pred)\n",
    "#     accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "models = [dt,rf,lg,MNB,BNB]\n",
    "\n",
    "for model in models:\n",
    "    predict_model(X,y,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
